{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ecbec89f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform the cleaned data into a purely numerical format suitable for machine learning algorithms,\n",
    "# and create new features to capture patterns identified during EDA.\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b2298839",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 1. DATA LOADING --- \n",
      " \n",
      "Loaded cleaned data with 7032 rows and 20 features.\n",
      "Features (X) shape: (7032, 19), Target (y) shape: (7032,)\n"
     ]
    }
   ],
   "source": [
    "# --- 1. DATA LOADING ---\n",
    "print(\"--- 1. DATA LOADING --- \\n \")\n",
    "try:\n",
    "    df = pd.read_csv('cleaned_telco_churn_data.csv')\n",
    "    print(f\"Loaded cleaned data with {df.shape[0]} rows and {df.shape[1]} features.\")\n",
    "except FileNotFoundError:\n",
    "    print(\"Error: 'cleaned_telco_churn_data.csv' not found. Please ensure the file is in the directory.\")\n",
    "    exit()\n",
    "\n",
    "# Separate features (X) and target (y)\n",
    "X = df.drop('Churn', axis=1)\n",
    "y = df['Churn']\n",
    "print(f\"Features (X) shape: {X.shape}, Target (y) shape: {y.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b75be4b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Categorical features to encode: ['gender', 'SeniorCitizen', 'Partner', 'Dependents', 'PhoneService', 'MultipleLines', 'InternetService', 'OnlineSecurity', 'OnlineBackup', 'DeviceProtection', 'TechSupport', 'StreamingTV', 'StreamingMovies', 'Contract', 'PaperlessBilling', 'PaymentMethod']\n"
     ]
    }
   ],
   "source": [
    "# Define feature types\n",
    "numerical_cols = ['tenure', 'MonthlyCharges', 'TotalCharges']\n",
    "categorical_cols = X.drop(columns=numerical_cols).columns.tolist()\n",
    "print(f\"Categorical features to encode: {categorical_cols}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d12815d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- 2. FEATURE CREATION: Tenure Grouping ---\n",
      "\n",
      "Created 'Tenure_Group' categorical feature based on 'tenure'\n",
      "\n",
      "New (0-1yr)        2058\n",
      "Mid (1-3yrs)       1923\n",
      "Senior (3-5yrs)    1568\n",
      "Loyal (5+yrs)      1483\n",
      "Name: Tenure_Group, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# --- 2. FEATURE CREATION (Binning Tenure) ---\n",
    "print(\"\\n--- 2. FEATURE CREATION: Tenure Grouping ---\\n\")\n",
    "# Based on EDA, tenure is highly bimodal and influential. We create bins for better interpretability\n",
    "# and to potentially improve non-linear model performance.\n",
    "\n",
    "bins = [0, 12, 36, 60, 100] # 0-1 year, 1-3 years, 3-5 years, 5+ years\n",
    "labels = ['New (0-1yr)', 'Mid (1-3yrs)', 'Senior (3-5yrs)', 'Loyal (5+yrs)']\n",
    "X['Tenure_Group'] = pd.cut(X['tenure'], bins=bins, labels=labels, right=False)\n",
    "\n",
    "# Drop the original tenure column after creating the binned feature.\n",
    "# We keep it for now but note that it might be dropped later if multicollinearity is an issue.\n",
    "print(\"Created 'Tenure_Group' categorical feature based on 'tenure'\\n\")\n",
    "print(X['Tenure_Group'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "790c2717",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- 3. CATEGORICAL ENCODING (One-Hot Encoding) ---\n",
      "Removed original 'tenure' to rely on binned feature and mitigate correlation with TotalCharges.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# --- 3. CATEGORICAL ENCODING (One-Hot Encoding) ---\n",
    "print(\"\\n--- 3. CATEGORICAL ENCODING (One-Hot Encoding) ---\")\n",
    "# Use pd.get_dummies to convert all categorical features into a set of binary (0 or 1) columns.\n",
    "# We use 'drop_first=True' to avoid multicollinearity within the dummy variables (the \"Dummy Variable Trap\").\n",
    "\n",
    "# Including the new Tenure_Group in the list of columns to encode\n",
    "cols_to_encode = categorical_cols + ['Tenure_Group']\n",
    "\n",
    "# Perform one-hot encoding\n",
    "X_encoded = pd.get_dummies(X, columns=cols_to_encode, drop_first=True)\n",
    "\n",
    "# Drop original 'tenure' if we proceed with 'Tenure_Group' as a primary feature\n",
    "# We will keep the original 'tenure' for now, as it is a strong predictor.\n",
    "X_encoded.drop(columns=['tenure'], inplace=True)\n",
    "print(\"Removed original 'tenure' to rely on binned feature and mitigate correlation with TotalCharges.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "33d6b5b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- 4. NUMERICAL SCALING ---\n",
      "\n",
      "Applied StandardScaler to 'MonthlyCharges' and 'TotalCharges'\n",
      "\n",
      "First 5 rows of scaled numerical features:\n",
      "\n",
      "   MonthlyCharges  TotalCharges\n",
      "0       -1.161694     -0.994194\n",
      "1       -0.260878     -0.173740\n",
      "2       -0.363923     -0.959649\n",
      "3       -0.747850     -0.195248\n",
      "4        0.196178     -0.940457\n"
     ]
    }
   ],
   "source": [
    "# --- 4. NUMERICAL SCALING ---\n",
    "print(\"\\n--- 4. NUMERICAL SCALING ---\\n\")\n",
    "# Scaling is crucial for distance-based and regularization-based models (Logistic Regression, SVM, kNN).\n",
    "# We only scale the truly continuous variables: MonthlyCharges and TotalCharges.\n",
    "\n",
    "# Define columns to scale\n",
    "cols_to_scale = ['MonthlyCharges', 'TotalCharges']\n",
    "\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Fit and transform the data\n",
    "X_encoded[cols_to_scale] = scaler.fit_transform(X_encoded[cols_to_scale])\n",
    "\n",
    "print(\"Applied StandardScaler to 'MonthlyCharges' and 'TotalCharges'\\n\")\n",
    "print(\"First 5 rows of scaled numerical features:\\n\")\n",
    "print(X_encoded[cols_to_scale].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "eba6072c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- 5. FINAL PREPARATION AND DATA SPLIT ---\n",
      "\n",
      "Final feature set (X_final) shape: (7032, 32)\n",
      "Final column list count: 32\n",
      "\n",
      "--- Train/Test Split Complete ---\n",
      "\n",
      "X_train shape: (5274, 32) | y_train Churn rate: 26.58%\n",
      "X_test shape: (1758, 32)   | y_test Churn rate: 26.56%\n",
      "\n",
      "Processed and split data saved to X_train.csv, X_test.csv, y_train.csv, and y_test.csv\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# --- 5. FINAL PREPARATION AND DATA SPLIT ---\n",
    "print(\"\\n--- 5. FINAL PREPARATION AND DATA SPLIT ---\\n\")\n",
    "\n",
    "# Align X and y\n",
    "X_final = X_encoded\n",
    "print(f\"Final feature set (X_final) shape: {X_final.shape}\")\n",
    "print(f\"Final column list count: {len(X_final.columns)}\")\n",
    "\n",
    "# Split the data into training and testing sets.\n",
    "# We use 'stratify=y' because the target variable ('Churn') is imbalanced.\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_final, y, test_size=0.25, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "print(\"\\n--- Train/Test Split Complete ---\\n\")\n",
    "print(f\"X_train shape: {X_train.shape} | y_train Churn rate: {y_train.mean():.2%}\")\n",
    "print(f\"X_test shape: {X_test.shape}   | y_test Churn rate: {y_test.mean():.2%}\")\n",
    "\n",
    "# Save processed data for the next step (Modeling)\n",
    "X_train.to_csv('X_train.csv', index=False)\n",
    "X_test.to_csv('X_test.csv', index=False)\n",
    "y_train.to_csv('y_train.csv', index=False, header=True)\n",
    "y_test.to_csv('y_test.csv', index=False, header=True)\n",
    "\n",
    "print(\"\\nProcessed and split data saved to X_train.csv, X_test.csv, y_train.csv, and y_test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28602c4c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
